{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"application_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataset.columns:\n",
    "    print(f\"Unique values in column '{column}':\")\n",
    "    print(dataset[column].nunique())\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_clean(app):\n",
    "    sanitized_app_name = app.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\").replace(\"/\", \"\").replace(\"\\\\\", \"\")\n",
    "    return sanitized_app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(data):\n",
    "    columns_to_drop = [\n",
    "        \"timestamp\",\"application_name\"\n",
    "    ]\n",
    "    \n",
    "    df=data.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "    \n",
    "    # Function to extract the port numbers\n",
    "    def extract_ports(domains_accessed):\n",
    "        # Split the domain-port pairs and extract the port number\n",
    "        ports = []\n",
    "        for domain in domains_accessed.split(','):\n",
    "            port = domain.split(':')[-1]  # Extract port number after ':'\n",
    "            ports.append(port)\n",
    "        return ports\n",
    "\n",
    "\n",
    "    # Apply the function to extract port numbers and transform the data\n",
    "    new_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row.isna().any():\n",
    "            new_row = row.drop(\"domains_accessed\")  # Remove the original domain column\n",
    "            new_row[\"connected_ports\"] = 0\n",
    "            new_rows.append(new_row)\n",
    "            continue\n",
    "\n",
    "        # Extract ports for the current row\n",
    "        ports = extract_ports(row[\"domains_accessed\"])\n",
    "        \n",
    "        # Create new rows with extracted port numbers\n",
    "        for port in ports:\n",
    "            new_row = row.drop(\"domains_accessed\")  # Remove the original domain column\n",
    "            new_row[\"connected_ports\"] = port \n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with the transformed data\n",
    "    transformed_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "CURRENT_PATH=os.getcwd()\n",
    "DATASET_PATH=CURRENT_PATH+\"\\\\dataset\"\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "\n",
    "#this is give a list of all the different app that are running \n",
    "app_list=dataset[\"application_name\"].unique().tolist()\n",
    "\n",
    "for app in app_list:\n",
    "    \n",
    "    if pd.isna(app):  # Skip missing values\n",
    "        continue\n",
    "    \n",
    "    app_data = dataset[dataset['application_name'] == app]\n",
    "    \n",
    "    app=name_clean(app)\n",
    "    app_data= data_clean(app_data)\n",
    "\n",
    "    os.makedirs(DATASET_PATH + f\"\\\\{app}\",exist_ok=True)\n",
    "    app_data.to_csv(DATASET_PATH + f\"\\\\{app}\"+f\"\\\\{app}.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CURRENT_PATH = os.getcwd()\n",
    "MODEL_PATH = CURRENT_PATH + \"\\\\model\"\n",
    "DETECTED_OUTPUT_PATH = os.path.join(CURRENT_PATH, \"detected_output\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "app_list = dataset[\"application_name\"].unique().tolist()\n",
    "\n",
    "# Add a function to plot anomalies based on anomaly scores\n",
    "def plot_anomalies(data_scaled, anomaly_scores, app_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(data_scaled)), anomaly_scores, c='blue', label='Normal', alpha=0.7)\n",
    "    plt.scatter(range(len(data_scaled)), anomaly_scores, c=anomaly_scores, cmap='coolwarm', label='Anomaly', alpha=0.7)\n",
    "    plt.title(f\"Anomalies detected for {app_name}\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Anomaly Score\")\n",
    "    plt.colorbar(label='Anomaly Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for app in app_list:\n",
    "    if pd.isna(app):  \n",
    "        continue\n",
    "    app = name_clean(app)\n",
    "    app_data = pd.read_csv(DATASET_PATH + f\"\\\\{app}\" + f\"\\\\{app}.csv\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(app_data)\n",
    " \n",
    "    # Train an anomaly detection model\n",
    "    model = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\n",
    "    model.fit(data_scaled)\n",
    "    \n",
    "    # Predict anomalies\n",
    "    app_data[\"Anomaly\"] = model.predict(data_scaled)\n",
    "    app_data[\"Anomaly\"] = app_data[\"Anomaly\"].map({1: \"Normal\", -1: \"Anomaly\"})\n",
    "    \n",
    "    # Evaluate the model: Get the anomaly scores\n",
    "    anomaly_scores = model.decision_function(data_scaled)\n",
    "    \n",
    "    # Plot anomalies\n",
    "    plot_anomalies(data_scaled, anomaly_scores, app)\n",
    "    \n",
    "    # Save the cleaned data with predictions\n",
    "    output = os.path.join(DETECTED_OUTPUT_PATH, f\"{app}\")\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    app_data.to_csv(os.path.join(output, f\"cleaned_data_with_predictions_{app}.csv\"), index=False)\n",
    "    \n",
    "    # Save the trained model\n",
    "    MODEL_APP_PATH = os.path.join(MODEL_PATH, f\"{app}\")\n",
    "    os.makedirs(MODEL_APP_PATH, exist_ok=True)\n",
    "    model_filename = os.path.join(MODEL_APP_PATH, f\"{app}_model.pkl\")\n",
    "    \n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "    \n",
    "    # Display a summary for the current app\n",
    "    print(f\"Summary of anomalies detected for {app}:\")\n",
    "    print(app_data[\"Anomaly\"].value_counts())\n",
    "    \n",
    "    # Display the anomaly score statistics (e.g., mean and std deviation)\n",
    "    print(f\"Anomaly scores for {app}:\")\n",
    "    print(f\"Mean anomaly score: {anomaly_scores.mean()}\")\n",
    "    print(f\"Std dev of anomaly score: {anomaly_scores.std()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
